
* Running 

Branch: def1

[skylake|spark]$ ./build/mvn  -DskipTests -Dcheckstyle.skip=true package -pl core

Takes <1 minute 

* Gen Spark 

There is now a PeriodicRDDCheckpointer!!!

/rdd/util/PeriodicRDDCheckpointer.scala

* UI Server location 

    attachHandler(createRedirectHandler(
      "/stages/stage/kill", "/stages/", stagesTab.handleKillRequest,
      httpMethods = Set("GET", "POST")))
in 

core/src/main/scala/org/apache/spark/ui/SparkUI.scala 

/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala 

* HTTP API Endpoint 

core/src/main/scala/org/apache/spark/status/api/v1/


ApiRootResource.scala : API endpoints registered using @Path("deflate") 
api.scala : class definitions 




* History Server 

Keep record of operation times to get recomputation time and other task details

/core/src/main/scala/org/apache/spark/deploy/history/


* Status 

Most of the useful metrics are collected in: 
core/src/main/scala/org/apache/spark/status/LiveEntity.scala

TaskInfo 

** AppStatusStore 

can create frp, AppStatusStore.createLiveStore(conf)

or use the one already created in sparkcontext.




* TODO 

 ::DONE::  - Get stages 
- Get current task information for each stage 
- Reclaim fraction parsing and logging 
- Infer shuffle using Wide/narrow dependency? 
- Executor killing/maintenance/black-list? 
- Executor respawning
::DONE:: - Num tasks remaining in a stage can be a good heuristic? 

